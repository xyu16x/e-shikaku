:toc:
:stem: latexmath

= 応用数学レポート

== 第1章 線形代数

=== 基礎
* 行列は数を表形式に並べたものスカラーは単なる数
* 連立方程式の解法に応用可能（加減法や逆行列を用いる）
* 逆行列は存在しない場合もあるA×B = 単位行列となる B が逆行列

=== 行列の計算

行列の加法::
[stem]
++++
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
+
\begin{bmatrix}
5 & 6 \\
7 & 8
\end{bmatrix}
=
\begin{bmatrix}
6 & 8 \\
10 & 12
\end{bmatrix}
++++

行列のスカラー倍::
[stem]
++++
2 \cdot
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
=
\begin{bmatrix}
2 & 4 \\
6 & 8
\end{bmatrix}
++++

行列の積::
* 行列の積 stem:[AB] は「行と列の内積」で定義
* 条件：A が m×n、B が n×p のとき、積 AB は m×p

[stem]
++++
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
\begin{bmatrix}
5 & 6 \\
7 & 8
\end{bmatrix}
=
\begin{bmatrix}
19 & 22 \\
43 & 50
\end{bmatrix}
++++

単位行列と逆行列::
* 単位行列 stem:[I] は「かけても変わらない行列」  
  stem:[AI = IA = A]
* 逆行列 stem:[A^{-1}] は stem:[AA^{-1} = A^{-1}A = I] を満たす  
  存在条件は det(A) ≠ 0

[stem]
++++
A^{-1} = \frac{1}{ad - bc}
\begin{bmatrix}
d & -b \\
- c & a
\end{bmatrix}
++++

行列式::
* 正方行列のみ定義される
* 逆行列の存在条件に関わる（行列式が 0 でないときに逆行列が存在）

[stem]
++++
\left| \begin{matrix}
a & b \\
c & d
\end{matrix} \right|
= ad - bc
++++



=== 固有値・固有ベクトル

定義::
正方行列stem:[A]に対して、以下の式が成立するような +
スカラー値stem:[\lambda]を**固有値**、非ゼロベクトルstem:[v]を**固有ベクトル** という

[stem]
++++
A v = \lambda v
++++

性質::
* 固有値は行列stem:[A]の次元分存在する
* 固有値stem:[\lambda]は「行列の伸縮率」を表す
* 固有ベクトルstem:[v]は「その伸縮方向」を表す
* 異なる固有値に対応する固有ベクトルは線形独立



=== 固有値分解

定義::
正方行列stem:[A]が固有値 stem:[\lambda_1, \lambda_2, ..., \lambda_n] と固有ベクトル stem:[v_1, v_2, ..., v_n] を持つとき以下のように変形することを**固有値分解**という
+
[stem]
++++
A = V \Lambda V^{-1}
++++
+
ただし、
+
[stem]
++++
\Lambda =
\begin{bmatrix}
\lambda_1 & 0 & \cdots & 0 \\
0 & \lambda_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \lambda_n
\end{bmatrix}
\quad
V = \begin{bmatrix}
v_1 & v_2 & \cdots & v_n
\end{bmatrix}
++++
+
* 行列の累乗や指数計算を効率化できる


具体例::
正方行列Aがstem:[
\quad
A = \begin{bmatrix}
4 & 1 \\
2 & 3
\end{bmatrix}
\quad
]のとき、
+
固有値は stem:[\lambda = 5, 2\quad]  固有ベクトルは
stem:[\quad
v_1 = \begin{bmatrix}1 \\ 1\end{bmatrix},\quad
v_2 = \begin{bmatrix}1 \\ -2\end{bmatrix}
]
+
固有値分解の一般形 stem:[A = V \Lambda V^{-1}] より、
stem:[
V = \begin{bmatrix}
1 & 1 \\
1 & -2
\end{bmatrix}, \quad
\Lambda = \begin{bmatrix}
5 & 0 \\
0 & 2
\end{bmatrix}
]


=== 特異値と特異ベクトル
定義::
任意のゼロ行列ではないstem:[m × n]行列Aに対して、以下の式が成立するような +
正の数stem:[\sigma]を**特異値**、m次元ベクトルstem:[u]を**左特異ベクトル**、n次元ベクトルstem:[v]を**右特異ベクトル**という
+
[stem]
++++
Av = σu, 
\quad
A^Tu = σv
++++
+
ただし、stem:[u, v]はともにゼロベクトルではないことが前提となる

計算手順::
. stem:[A^TA] の固有値を求める
. その平方根を特異値とする
. 固有ベクトルから右特異ベクトル stem:[v] を構成
. 左特異ベクトル stem:[u は Av_i = σ_i u_i] から求める

=== 特異値分解
定義::
行列Aに対して、特異値と特異ベクトルを持つ場合に以下の式で特異値分解できる
+
[stem]
++++
A = U \Sigma V^{T}
++++
+
* U：m×m の直交行列 （列ベクトルは左特異ベクトル） stem:[\quad U = \begin{bmatrix}u_1 \quad u_2 \quad ... \quad u_r \end{bmatrix}]
* V：n×n の直交行列 （列ベクトルは右特異ベクトル） stem:[\quad V = \begin{bmatrix}v_1 \quad v_2 \quad ... \quad v_r \end{bmatrix}]
* Σ：m×n の対角行列（対角成分は特異値、非負で降順に並ぶ）
stem:[
\Sigma =
\begin{bmatrix}
\sigma_1 & 0        & \cdots & 0 \\
0        & \sigma_2 & \cdots & 0 \\
\vdots   & \vdots   & \ddots & \vdots \\
0        & 0        & \cdots & \sigma_r
\end{bmatrix}
]


固有値分解との違い::
[cols="1,2,2",options="header"]
|===
| 項目 | 固有値分解 | 特異値分解
| 適用範囲 | 正方行列のみ | 任意の m×n 行列
| 値の性質 | 固有値は負もあり得る | 特異値は常に非負
| 幾何学的意味 | 固有ベクトル方向の伸縮率 | 入力→出力の回転＋伸縮＋回転
| 主な応用 | 安定性解析、PCA基礎 | 次元削減、画像圧縮、推薦システム
|===


== 第2章 確率・統計

=== 基礎
集合::
要素の集まり
+
集合 stem:[S] に含まれる要素は次のように表す：
+
[stem]
++++
S = \{a, b, c, d, e, f, g\}, \quad a \in S (a が集合 S の要素である)
++++
+
集合 stem:[S] の内部に集合 stem:[M] がある場合は次のように表す：
+
[stem]
++++
M = \{c, d, g\}, \quad M \subset S (集合 M が集合 S の部分集合)
++++
確率::
* 頻度確率（客観確率）：発生する頻度、誰が計算しても同じ値
* ベイズ確率（主観確率）：人の信念や判断の度合い、人によって異なる値（火星人がいる確率）
+
[stem]
++++
P(A) = \frac{n(A)}{n(U)} = \frac{事象Aが起こる数}{すべての事象の数}
++++

=== 条件付き確率と同時確率

条件付き確率::
stem:[ある事象 B が起きた条件下で A となる確率] +
stem:[⇒すでに事象B]が起きた世界に限定(**条件付き**)して考えるため、stem:[AとBが同時に起こる確率 P(A \cap B)]より確率が高くなる

[stem]
++++
P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{P(A \cap B)}{n(B)}
++++

同時確率::
お互いの発生には因果関係のない事象Aと事象Bが同時に発生する確率
[stem]
++++
P(A \cap B) = P(A)P(B|A) = P(A)P(B) = P(B \cap A)
++++

=== ベイズの法則
* 事後確率を求める基本公式
* 機械学習の推論に不可欠

[stem]
++++
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
++++

=== 確率変数と確率分布

確率変数::
* 確率的に値が決まる変数  
* サイコロの出目、身長、気温などが例として挙げられる  

確率分布::
* 確率変数がどの値をどれくらいの確率で取るかを表すルール  
* 離散値では確率質量関数 stem:[P(X=x)]
* 連続値では確率密度関数 stem:[f_X(x)] 
+
.離散値の例（サイコロ）
[cols="1,1,1,1,1,1,1",options="header"]
|===
| 出目 (x) | 1 | 2 | 3 | 4 | 5 | 6
| 確率 P(X=x) | 1/6 | 1/6 | 1/6 | 1/6 | 1/6 | 1/6
|===

確率::
* ある値や範囲が起こる割合
* 確率密度そのものは確率ではない⇒積分して確率になる
+
.離散値の場合
[stem]
++++
P(X=x)
++++
+
.連続値の場合
[stem]
++++
P(a \le X \le b) = \int_a^b f_X(x)\, dx
++++

期待値 stem:[E[f(X)]]::
* 確率変数の重み付き平均
* 多数回試行すると、その平均が期待値に近づく（大数の法則）
* 実際に出る値とは限らない（サイコロの期待値3.5など）
+
.離散値の場合
[stem]
++++
E[X] = \sum_{k=1}^{n} x_k \, P(X = x_k)
++++
+
.連続値の場合
[stem]
++++
E[X] = \int_{-\infty}^{\infty} x \, f_X(x) \, dx
++++

=== 分散・共分散・標準偏差
分散::
* データの散らばりの平均（確率や密度を掛けているため散らばりの大きさではない） +
* 各々のデータが期待値からどれだけ離れているか +
* 期待値を基準にした平均二乗誤差
+
[stem]
++++
\mathrm{Var}(X)
= E[(X - E[X])^2]
= E[f(X)^2] - \left( E[f(X)] \right)^2
++++


共分散::
* 2つの確率変数が同じ方向に動くか、逆方向に動くかを測る量
* 共分散が正：一緒に動く
* 共分散が負：逆に動く
* 共分散が0：関係性がない
+
[stem]
++++
\mathrm{Cov}(X, Y)
= E[(X - E[X])(Y - E[Y])]
= E[XY] - E[X]E[Y]
++++


標準偏差::
* 分散の平方根
+
[stem]
++++
\mathrm{SD}(f(X))
= \sqrt{ \mathrm{Var}(f(X)) }
= \sqrt{ E\left[ \left( f(X) - E[f(X)] \right)^2 \right] }
++++


=== 様々な確率分布

ベルヌーイ分布::
* 「成功 or 失敗」など、2つの結果しかない確率変数の分布
* コインの表裏、スパムメール非スパムメールなど
+
[stem]
++++
P(x \mid \mu) = \mu^x (1 - \mu)^{1 - x},\quad x \in \{0, 1\}
++++

マルチヌーイ分布::
* K個のカテゴリのうち1つが選ばれる分布
* ベルヌーイ分布の多値、K=2のマルチヌーイ分布がベルヌーイ分布となる
+
[stem]
++++
P(\boldsymbol{x} \mid \boldsymbol{p}) = \prod_{i=1}^{K} p_i^{x_i}
++++

二項分布::
* ベルヌーイ試行をn回繰り返したときの成功回数の分布
+
[stem]
++++
P(x \mid \lambda, n)
= \frac{n!}{x!(n - x)!} \lambda^x (1 - \lambda)^{n - x}
++++
+
** stem:[x]：成功回数（0〜n）
** stem:[n]：試行回数
** stem:[\lambda]：1回の試行における成功確率

ガウス分布::
* 釣鐘型の連続分布、正規分布とも呼ばれる
* 多くの自然現象が従う
+
[stem]
++++
\mathcal{N}(x; \mu, \sigma^2)
= \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(x - \mu)^2}{2\sigma^2} \right)
++++
+
** stem:[x]：連続値の確率変数
** stem:[\mu]：平均（期待値）
** stem:[\sigma^2]：分散

=== 推定

推定::
* 標本から母集団の特徴（平均・分散など）を数学的に予測すること
** 点推定：1つの値で母数を予測する
** 区間推定：ある範囲（信頼区間）で母数を予測する

母集団::
* 本来知りたい全体（例：日本人全員の身長）

標本::
* 実際に観測できる一部（例：100人の身長）

標本平均::
サンプル数が大きくなれば母集団の値に近づく（一致性）
サンプル数がいくらであってもその期待値は母集団の値と同様（不偏性）

推定量と推定値::
* 推定量：標本から計算される関数
* 推定値：実際に得られた数値

標本分散::
* 標本平均 stem:[\bar{x}] を使って、データのばらつきを測る
* 分母が stem:[n] なので、母分散の推定としては過小評価になる傾向がある
* 「標本分散」と呼ばれるが、母分散の不偏推定量ではない
+
[stem]
++++
\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2
++++

不偏分散::
* 分母が stem:[n - 1] になっているのが特徴、自由度の補正による
* この補正により、母分散の不偏推定量になる
* 統計学ではこちらを「標本分散」と呼ぶこともあるが、厳密には「不偏分散」
+
[stem]
++++
s^2 = \frac{1}{n - 1} \sum_{i=1}^{n} (x_i - \bar{x})^2
++++

== 第3章 情報理論

=== 自己情報量
* 確率変数がとる一つの結果がどれだけ**意外か**を数値化したもの
* 確率が低い事象ほど情報量が大きい
* 確率1の事象の情報量は0 stem:[I = - \log 1 = 0]
* 対数の底が2のとき、単位はビット
* 対数の底がネイピアのstem:[e]のとき、単位はnat
+
[stem]
++++
I(x) = -\log P(x) = \log W(x)
++++
+
.コイン投げと宝くじの例
[stem]
++++
I(\text{コイン表}) = -\ln(0.5) = 0.693 \\
I(\text{宝くじ1等}) = -\ln(10^{-7}) = 16.1
++++

=== シャノンエントロピー
* 確率分布全体の**平均的な意外性**
* 自己情報量の期待値
+
[stem]
++++
H(X) = E(x) = -E(\log P(x)) = -\sum_i P(x_i)\log P(x_i)
++++

=== KLダイバージェンス(カルバックライブラーダイバージェンス)
* 確率分布stem:[P]とstem:[Q]の違いを測る指標
* ある事象 stem:[x] に対して、分布 stem:[P(x)] と stem:[Q(x)] の自己情報量の差は次のように表される
+
[stem]
++++
I(Q(x)) - I(P(x)) 
= (-\log Q(x)) - (-\log P(x)) 
= \log \frac{P(x)}{Q(x)}
++++
+
** stem:[I(P(x)) = -\log P(x)]：本来の情報量
** stem:[I(Q(x)) = -\log Q(x)]：近似分布の情報量
** 差分は「本来よりどれだけ余計に情報が必要か」
+
* この差を全体の期待値として集約すると、KLダイバージェンスになる
+
[stem]
++++
D_{\mathrm{KL}}(P \,\|\, Q)
= \sum_x P(x) \left( -\log Q(x) \right) - \left( -\log P(x) \right)
= \sum_x P(x) \log \frac{P(x)}{Q(x)}
++++

=== 交差エントロピー
* 分布 stem:[P(x)] に従って生じる事象を、近似分布 stem:[Q(x)] で符号化したときの**平均情報量**を表す
+
[stem]
++++
H(P, Q) = - \sum_x P(x) \log Q(x)
++++

期待値としての表現::
+
[stem]
++++
H(P, Q) = -\mathbb{E}_{x \sim P} \log Q(x)
++++

エントロピーとの関係::
+
[stem]
++++
H(P, Q) = H(P) + D_{\mathrm{KL}}(P \,\|\, Q)
++++


== 参考資料

=== 第1章 線形代数
* https://math-notes.info/li/[大学数学の授業ノート：線形代数]
* https://www.rimath.saitama-u.ac.jp/lab.jp/Fukui/lectures/Linear_algebra.pdf[埼玉大学「線形代数学講義ノート」]

=== 第2章 確率・統計
* https://www.stat.go.jp/dss/[統計数理研究所「統計WEB」]
* https://ocw.kyoto-u.ac.jp/course/809/[エレベータのブザーは鳴るか―大学生のための統計学入門]

=== 第3章 情報理論
* https://www-int.ist.osaka-u.ac.jp/user/watanabe/[大阪大学「情報理論・符号理論の基礎」]
* https://sites.google.com/view/toyoakinishida-j/%E6%83%85%E5%A0%B1%E7%AC%A6%E5%8F%B7%E7%90%86%E8%AB%96[情報符号理論]
