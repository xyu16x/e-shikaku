= ラビットチャレンジ応用数学レポート
:toc:
:stem: latexmath

== 第1章 線形代数

=== 基礎
* 行列は数を表形式に並べたもの。スカラーは単なる数
* 連立方程式の解法に応用可能（加減法や逆行列を用いる）
* 逆行列は存在しない場合もある。A×B = 単位行列となる B が逆行列

=== 行列の計算

行列の加法::
[stem]
++++
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
+
\begin{bmatrix}
5 & 6 \\
7 & 8
\end{bmatrix}
=
\begin{bmatrix}
6 & 8 \\
10 & 12
\end{bmatrix}
++++

行列のスカラー倍::
[stem]
++++
2 \cdot
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
=
\begin{bmatrix}
2 & 4 \\
6 & 8
\end{bmatrix}
++++

行列の積::
* 行列の積 stem:[AB] は「行と列の内積」で定義
* 条件：A が m×n、B が n×p のとき、積 AB は m×p

[stem]
++++
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
\begin{bmatrix}
5 & 6 \\
7 & 8
\end{bmatrix}
=
\begin{bmatrix}
19 & 22 \\
43 & 50
\end{bmatrix}
++++

単位行列と逆行列::
* 単位行列 stem:[I] は「かけても変わらない行列」  
  stem:[AI = IA = A]
* 逆行列 stem:[A^{-1}] は stem:[AA^{-1} = A^{-1}A = I] を満たす  
  存在条件は det(A) ≠ 0

[stem]
++++
A^{-1} = \frac{1}{ad - bc}
\begin{bmatrix}
d & -b \\
- c & a
\end{bmatrix}
++++

行列式::
* 正方行列のみ定義される
* 逆行列の存在条件に関わる（行列式が 0 でないときに逆行列が存在）

[stem]
++++
\left| \begin{matrix}
a & b \\
c & d
\end{matrix} \right|
= ad - bc
++++



=== 固有値・固有ベクトル

定義::
正方行列stem:[A]に対して、以下の式が成立するような +
スカラー値stem:[\lambda]を**固有値**、非ゼロベクトルstem:[v]を**固有ベクトル** という

[stem]
++++
A v = \lambda v
++++

性質::
* 固有値は行列stem:[A]の次元分存在する
* 固有値stem:[\lambda]は「行列の伸縮率」を表す
* 固有ベクトルstem:[v]は「その伸縮方向」を表す
* 異なる固有値に対応する固有ベクトルは線形独立



=== 固有値分解

定義::
正方行列stem:[A]が固有値 stem:[\lambda_1, \lambda_2, ..., \lambda_n] と固有ベクトル stem:[v_1, v_2, ..., v_n] を持つとき以下のように変形することを**固有値分解**という
+
[stem]
++++
A = V \Lambda V^{-1}
++++
+
ただし、
+
[stem]
++++
\Lambda =
\begin{bmatrix}
\lambda_1 & 0 & \cdots & 0 \\
0 & \lambda_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \lambda_n
\end{bmatrix}
\quad
V = \begin{bmatrix}
v_1 & v_2 & \cdots & v_n
\end{bmatrix}
++++
+
* 行列の累乗や指数計算を効率化できる


具体例::
正方行列Aがstem:[
\quad
A = \begin{bmatrix}
4 & 1 \\
2 & 3
\end{bmatrix}
\quad
]のとき、
+
固有値は stem:[\lambda = 5, 2\quad]  固有ベクトルは
stem:[\quad
v_1 = \begin{bmatrix}1 \\ 1\end{bmatrix},\quad
v_2 = \begin{bmatrix}1 \\ -2\end{bmatrix}
]
+
固有値分解の一般形 stem:[A = V \Lambda V^{-1}] より、
stem:[
V = \begin{bmatrix}
1 & 1 \\
1 & -2
\end{bmatrix}, \quad
\Lambda = \begin{bmatrix}
5 & 0 \\
0 & 2
\end{bmatrix}
]


=== 特異値と特異ベクトル
定義::
任意のゼロ行列ではないstem:[m × n]行列Aに対して、以下の式が成立するような +
正の数stem:[\sigma]を**特異値**、m次元ベクトルstem:[u]を**左特異ベクトル**、n次元ベクトルstem:[v]を**右特異ベクトル**という
+
[stem]
++++
Av = σu, 
\quad
A^Tu = σv
++++
+
ただし、stem:[u, v]はともにゼロベクトルではないことが前提となる

計算手順::
. stem:[A^TA] の固有値を求める
. その平方根を特異値とする
. 固有ベクトルから右特異ベクトル stem:[v] を構成
. 左特異ベクトル stem:[u は Av_i = σ_i u_i] から求める

=== 特異値分解
定義::
行列Aに対して、特異値と特異ベクトルを持つ場合に以下の式で特異値分解できる
+
[stem]
++++
A = U \Sigma V^{T}
++++
+
* U：m×m の直交行列 （列ベクトルは左特異ベクトル） stem:[\quad U = \begin{bmatrix}u_1 \quad u_2 \quad ... \quad u_r \end{bmatrix}]
* V：n×n の直交行列 （列ベクトルは右特異ベクトル） stem:[\quad V = \begin{bmatrix}v_1 \quad v_2 \quad ... \quad v_r \end{bmatrix}]
* Σ：m×n の対角行列（対角成分は特異値、非負で降順に並ぶ）
stem:[
\Sigma =
\begin{bmatrix}
\sigma_1 & 0        & \cdots & 0 \\
0        & \sigma_2 & \cdots & 0 \\
\vdots   & \vdots   & \ddots & \vdots \\
0        & 0        & \cdots & \sigma_r
\end{bmatrix}
]


固有値分解との違い::
[cols="1,2,2",options="header"]
|===
| 項目 | 固有値分解 | 特異値分解
| 適用範囲 | 正方行列のみ | 任意の m×n 行列
| 値の性質 | 固有値は負もあり得る | 特異値は常に非負
| 幾何学的意味 | 固有ベクトル方向の伸縮率 | 入力→出力の回転＋伸縮＋回転
| 主な応用 | 安定性解析、PCA基礎 | 次元削減、画像圧縮、推薦システム
|===


== 第2章 確率・統計

=== 基礎
集合::
要素の集まり
+
集合 stem:[S] に含まれる要素は次のように表す：
+
[stem]
++++
S = \{a, b, c, d, e, f, g\}, \quad a \in S (a が集合 S の要素である)
++++
+
集合 stem:[S] の内部に集合 stem:[M] がある場合は次のように表す：
+
[stem]
++++
M = \{c, d, g\}, \quad M \subset S (集合 M が集合 S の部分集合)
++++
確率::
* 頻度確率（客観確率）：発生する頻度、誰が計算しても同じ値
* ベイズ確率（主観確率）：人の信念や判断の度合い、人によって異なる値（火星人がいる確率）
* 条件付き確率：ある事象が起きた条件下での確率
+
[stem]
++++
P(A) = \frac{n(A)}{n(U)} = \frac{事象Aが起こる数}{すべての事象の数}
++++

=== ベイズの法則

[stem]
++++
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
++++

* 事後確率を求める基本公式
* 機械学習の推論に不可欠

=== 確率変数・期待値

* サイコロの出目の期待値は出目×確率の総和
[stem]
++++
E[X] = \sum_i x_i P(x_i)
++++

=== 分散・標準偏差

[stem]
++++
Var(X) = E[X^2] - (E[X])^2
++++

* 分散の平方根が標準偏差
* データのばらつきを表す

== 第3章 情報理論

=== 自己情報量
[stem]
++++
I(x) = -\log P(x)
++++

* 確率が低い事象ほど情報量が大きい

=== シャノンエントロピー
[stem]
++++
H(X) = -\sum_i P(x_i)\log P(x_i)
++++

* 自己情報量の期待値

=== KLダイバージェンス
[stem]
++++
D_{KL}(P||Q) = \sum_i P(x_i)\log \frac{P(x_i)}{Q(x_i)}
++++

* 確率分布 P と Q の違いを測る指標

=== 交差エントロピー
[stem]
++++
H(P,Q) = -\sum_i P(x_i)\log Q(x_i)
++++

* ある分布を別の分布で近似したときの平均情報量

== 考察

* 線形代数は機械学習・ディープラーニングの基盤であり、固有値分解は次元削減や安定性解析に直結する
* 確率統計はデータ分析の根幹であり、ベイズの法則や期待値の理解が推論に不可欠
* 情報理論は機械学習の損失関数や通信理論に応用され、エントロピーやKLダイバージェンスはモデル評価に重要
* さらに、**線形代数と確率統計を情報理論と結びつける視点**が重要例えば、PCAは共分散行列の固有値分解を通じて情報量を最大化する方向を見抜く
* このように三分野は独立ではなく、**機械学習の理論的基盤として相互に補完し合う**
