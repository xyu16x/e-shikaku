:toc:
:stem: latexmath

= 機械学習レポート

== 第1章 機械学習の課題

=== 機械学習とは
様々なデータに潜むルールやパターンを学習し、未知のデータを予測・分析する +
コンピューターに大量のデータを楽手8宇させることでルールやパターン +

=== 誤差
訓練誤差::
訓練データに対する予測と正解の誤差

汎化誤差::
未知のデータに対する予測と正解の誤差

=== 過学習
オーバーフィッティング(過学習)::
* 過学習、汎化性能×
* バリアンスが高い

アンダーフィッティング::
* 未学習
* バイアスが高い

=== 機械学習モデリングプロセス
. 問題設定（一番重要） +
データ数は足りるか、ルールベースでは問題があるか
. データ選定
. データの前処理（EDA） +
全体のプロセスの中で時間をとられる、Kaggleで学べる +
欠損、入力ミス、外れ値の処理
. 機械学習、モデルの選定 +
線形回帰、ロジ回帰、k-means、SVM、PCA、knn
. モデルの学習
. モデルの評価

=== 機械学習モデル
[cols="1,1,1,1,1", options="header"]
|===
| 学習種類 | タスク | 機械学習モデル | パラメータの推定問題 | モデル選択・評価

| 教師あり学習
| 予測
| 線形回帰・非線形回帰
| 最小2乗法・尤度最大化
| ホールドアウト法

| 教師あり学習
| 分類
| ロジスティック回帰
| 尤度最大化（最尤法）
| 交差検証法

| 教師あり学習
| 分類
| 最近傍・K-近傍アルゴリズム
| 尤度最大化（最尤法）
| 交差検証法

| 教師あり学習
| 分類
| サポートベクターマシン
| マージン最大化
| 交差検証法

| 教師なし学習
| クラスタリング
| K-meansアルゴリズム
| 尤度最大化（最尤法）
| 無し

| 教師なし学習
| 次元削減
| 主成分分析
| 分散最大化
| 無し
|===

== 第2章 性能指標
=== 性能指標まとめ
[cols="1,2,2,2", options="header"]
|===
| 項目 | 2値分類 | 多値分類 | 回帰タスク

| 予測対象
| 種類（2クラス）
| 種類（3クラス以上）
| 連続値

| 評価基準
| 正解した割合
| 正解した割合
| 正解との誤差

| 性能指標
| 混同行列 +
ROC曲線 +
AUC +
正解率 +
適合率 +
再現率 +
F値
| 混同行列 +
正解率 +
micro平均 +
macro平均
| MSE +
RMSE +
MAE +
決定係数（\(R^2\)）
|===

分類::
正しく分類できたかを評価

回帰::
実測値との誤差を評価

=== 混同行列
[cols="1,1,1", options="header"]
[cols="1,1,1", options="header"]
|===
| 実際の値 / 予測値 
| 陽性 (Positive)
| 陰性 (Negative)

| 陽性 (Positive)
| 真陽性 **TP**
| 偽陰性 **FN**

| 陰性 (Negative)
| 偽陽性 **FP**
| 真陰性 **TN**
|===


[cols="1,2,2,2", options="header"]
|===
| 指標名 | 定義・意味 | 式（LaTeX） | 適用場面・注意点
| Accuracy +
（正解率）
| 全体のうち正しく分類された割合
| クラス分布が均等な場合に有効
| 
\[
 \frac{TP + TN}{TP + TN + FP + FN}
\]

| Precision +
（適合率）
| 陽性と予測した中で、実際に陽性だった割合
| 偽陽性を避けたい場面 +
（例：スパム検出）
| 
\[
\frac{TP}{TP + FP}
\]

| Recall +
（再現率）
| 実際に陽性だった中で、陽性と予測できた割合
| 偽陰性を避けたい場面 +
（例：病気検出）
| 
\[
\frac{TP}{TP + FN}
\]
| F1スコア
| Precision と Recall の調和平均
| クラス不均衡時に有効
| 
\[
\frac{2PR}{P + R}
\]
|===


=== ROC曲線
image::https://copilot.microsoft.com/th/id/BCO.b25b4a86-4a82-44d3-9931-97df7e1daf0a.png[ROC曲線, width=400, align=center]

ROC曲線::
* モデルが出力するスコアに対して、閾値を0〜1の間で変化させながらTPRとFPRを計算して曲線にする
** 閾値が低い→すべて陽性判定→TPRもFPRも高い
** 閾値が高い→ほとんど陰性判定→TPRもFPRも低い
* 2値分類の正確性を評価できる
* クラス不均衡に強い

AUC(Area Under Curve)::
* ROC曲線の下の面積（0〜1）を AUC と呼ぶ
** AUC = 1.0 → 完全に正しく分類できるモデル
** AUC = 0.5 → ランダム予測と同じ（性能なし）
** AUC < 0.5 → 逆に分類してる（ラベル反転すれば改善）

=== micro平均、macro平均
[cols="1,2,2,2", options="header"]
|===
| 手法 | 定義 | 式 | 特徴・適用場面

| micro平均
| 全クラスの TP, FP, FN を +
合算してから計算する  
| 

\[
\text{Precision}_{micro} = \frac{\sum TP}{\sum TP + \sum FP}
\]


\[
\text{Recall}_{micro} = \frac{\sum TP}{\sum TP + \sum FN}
\]


| ・多数派クラスに影響されやすい +
・全体の分類精度を評価したいときに有効

| macro平均
| 各クラスごとに +
Precision, Recall を計算し、 +
その平均を取る  
| 

\[
\text{Precision}_{macro} = \frac{1}{K} \sum_{k=1}^{K} \text{Precision}_k
\]

\[
\text{Recall}_{macro} = \frac{1}{K} \sum_{k=1}^{K} \text{Recall}_k
\]

| ・すべてのクラスを均等に評価 +
・少数派クラスの性能も重視したいときに有効
|===

=== 回帰タスクの性能指標
[cols="1,2,2", options="header"]
|===
| 指標名 | 定義・意味 | 特徴

| MSE (平均二乗誤差, 残差平方和) +
(Mean Squared Error)
|


\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]


|
・0に近いほど精度が高い +
・誤差を二乗するため **大きな誤差を強く罰する** +
・外れ値に敏感 +
・最適化で扱いやすく、理論的性質が良い（微分可能）

| RMSE (二乗平均平方根誤差) +
(Root Mean Squared Error)
|
\[
\text{RMSE} = \sqrt{\text{MSE}}
\]


|
・0に近いほど精度が高い +
・MSE の平方根で **元のスケールに戻した指標** +
・解釈しやすい +
・外れ値に敏感なのは MSE と同じ

| MAE (平均絶対誤差) +
(Mean Absolute Error)
| 
\[
\text{MAE} =   \frac{1}{n} \sum_{i=1}^{n} abs(y_i - \hat{y}_i)
\]

a| ・0に近いほど精度が高い + ・誤差の絶対値の平均 + ・外れ値に強い（ロバスト） + ・L1損失に対応

| 決定係数 (R², Coefficient of Determination)
|


\[
R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
\]


|
・1に近いほど精度が高い +
・モデルがどれだけデータを説明できているか +
・0以下になることもある（平均予測より悪い場合）
|===

== 第3章 線形回帰モデル
=== 回帰とは
線形::
* n次元空間における超平面の方程式
* ベクトルで表すことも可能

回帰問題::
* ある入力（離散値または連続値）から出力（連続値）を予測する問題
** 線形回帰：直線
** 非線形回帰：曲線

=== 特徴量表現

説明変数::
+
stem:[
\mathbf{x} = (x_1, x_2, \dots, x_m)^T \in \mathbb{R}^m
]
+
* m次元のベクトル
* 回帰モデルが**入力**として受け取る値
* 予測に影響を与える要因であり、数値・カテゴリ・時系列など多様な形式を取る

目的変数::
+
stem:[
y \in \mathbb{R}
]
+
* モデルが最終的に予測したい値
* 回帰問題では連続値を取ることが特徴
* 回帰モデルは、説明変数 x から目的変数 y を予測する関数 f を学習するstem:[\quad y = f(x)]

=== 線形回帰モデル
* 回帰問題を解くための機械学習モデルの一つ +
* 教師アリ学習stem:[\quad \{(x_i, y_i)\;|\;i = 1, \ldots, n\}] +
* 入力とm次元パラメータの線形結合を出力するモデル

// リスト終了を明示 
[none]

パラメータ::
stem:[\mathbf{w} = (w_1, w_2, \cdots, w_m)^T \in \mathbb{R}^m]
- 各特徴量に対応する重み
- ベクトル形式で表現され、学習によって最適化される


予測値::
stem:[\hat{y} = \mathbf{w}^T \mathbf{x} + w_0 = \sum_{j=1}^{m} w_j x_j + w_0] +
* 入力ベクトル stem:[\mathbf{x}] と重み stem:[\mathbf{w}] の内積にバイアス項 stem:[w_0] を加えたもの 
* stem:[\hat{y}] は予測値（ハット付きで表記）

=== 説明変数が1次元の場合(単回帰モデル)
* データへの過程：データは回帰直線に誤差が加わり観測されていると仮定

// リスト終了を明示 
[none]

モデル数式::
+
[stem]
++++
\underbrace{y}_{\text{目的変数}}
=
\underbrace{w_0}_{\text{切片}}
+
\underbrace{w_1}_{\text{回帰係数}}
\underbrace{x_1}_{\text{説明変数}}
+
\underbrace{\epsilon}_{\text{誤差}}
++++
+
** stem:[x, y]：既知、入力データ
** stem:[w]：未知、学習する
** 目的変数の予測に必要な説明変数をモデルに含めていない場合、その説明変数の影響は誤差に含まれる

連立方程式::
+
* 各データをモデル式へ代入すると、n個の式が導出される
+
[stem] 
++++
y_1 = w_0 + w_1 x_1 + \epsilon_1 \\ y_2 = w_0 + w_1 x_2 + \epsilon_2 \\ \vdots \\ y_n = w_0 + w_1 x_n + \epsilon_n
++++

行列表現::
[stem]
++++
\underbrace{
\begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{pmatrix}
}_{n \times 1}
=
\underbrace{
\begin{pmatrix}
1 & x_1 \\
1 & x_2 \\
\vdots & \vdots \\
1 & x_n
\end{pmatrix}
}_{n \times 2}
\;
\underbrace{
\begin{pmatrix}
w_0 \\
w_1
\end{pmatrix}
}_{2 \times 1}
++++


=== 説明変数が多次元の場合(重回帰モデル)
* 行列でわからない場合は、連立方程式に変換して考える
* データ数stem:[n>m+1]でないと解けない

// リスト終了を明示 
[none]

連立方程式::
+
[stem]
++++
y_1 = w_0 + w_1 x_{11} + w_2 x_{12} + \cdots + w_m x_{1m} + \epsilon_1 \\
y_2 = w_0 + w_1 x_{21} + w_2 x_{22} + \cdots + w_m x_{2m} + \epsilon_2 \\
\vdots \\
y_n = w_0 + w_1 x_{n1} + w_2 x_{n2} + \cdots + w_m x_{nm} + \epsilon_n
++++

行列表現::
+
[stem]
++++
\underbrace{
\begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{pmatrix}
}_{n \times 1}
=
\underbrace{
\begin{pmatrix}
1 & x_{11} & x_{12} & \cdots & x_{1m} \\
1 & x_{21} & x_{22} & \cdots & x_{2m} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \cdots & x_{nm}
\end{pmatrix}
}_{n \times (m+1)}
\;
\underbrace{
\begin{pmatrix}
w_0 \\
w_1 \\
\vdots \\
w_m
\end{pmatrix}
}_{(m+1) \times 1}
+
\underbrace{
\begin{pmatrix}
\epsilon_1 \\
\epsilon_2 \\
\vdots \\
\epsilon_n
\end{pmatrix}
}_{n \times 1}
++++


=== 汎化

モデルの容量::
* パラメータ数
* 表現力
* 関数クラスの複雑さ

データ量とデータ分布::
+
* データ数が多いほど汎化しやすい
* 訓練データとテストデータの分布が一致していることが前提 +
※分布がずれると**データシフト**汎化性能が落ちる

正則化::
+
* 汎化をよくするための技術
* モデルの自由度を制御し、過学習を防ぐ
** L1正則化(Lasso)
** L2正則化(Ridge)
** 早期終了
** ドロップアウト
** データ拡張


=== 誤差
平均二乗誤差::
* データとモデル出力の二乗誤差の和
* パラメータのみに依存する関数

最小二乗法::
* 線形回帰モデルのパラメータは最小二乗法で推定
* 学習データの平均二乗誤差を最小とするパラメータを探索
* 学習データの平均二乗誤差の最小化はその勾配が0になる点を求めればよい
+
[stem]
++++
\mathrm{MSE}_{\mathrm{train}} =
\frac{1}{n_{\mathrm{train}}}
\sum_{i=1}^{n_{\mathrm{train}}}
\left( \hat{y}_i^{(\mathrm{train})} - y_i^{(\mathrm{train})} \right)^2
++++

* 二乗損失は外れ値に弱い +
→Huber損失、Tukey損失などのロバスト損失関数を使用することも


最小二乗法による回帰係数の導出::
+
stem:[\hat{\mathbf{w}} = \arg\min_{\mathbf{w} \in \mathbb{R}^{m+1}} \mathrm{MSE}_{\mathrm{train}} \quad \text{(MSEを最小にする } m \text{次元の } \mathbf{w} \text{)}] +
+
stem:[\frac{\partial}{\partial \mathbf{w}} \mathrm{MSE}_{\mathrm{train}} = 0 \quad \text{(MSEを} \mathbf{w} \text{に関して微分したものが} 0 \text{となる} \mathbf{w} \text{の点を求める)}] +
+
stem:[\Rightarrow \frac{\partial}{\partial \mathbf{w}} \left\{ \frac{1}{n_{\mathrm{train}}} \sum_{i=1}^{n_{\mathrm{train}}} \left( \hat{y}_i^{(\mathrm{train})} - y_i^{(\mathrm{train})} \right)^2 \right\} = 0] +
+
stem:[ = \frac{1}{n_{\mathrm{train}}} \frac{\partial}{\partial \mathbf{w}} \left\{ \left( X^{(\mathrm{train})} \mathbf{w} - \mathbf{y}^{(\mathrm{train})} \right)^T \left( X^{(\mathrm{train})} \mathbf{w} - \mathbf{y}^{(\mathrm{train})} \right) \right\}] +
+
stem:[ = \frac{1}{n_{\mathrm{train}}} \frac{\partial}{\partial \mathbf{w}} \left\{ \mathbf{w}^T X^{(\mathrm{train})T} X^{(\mathrm{train})} \mathbf{w} - 2 \mathbf{w}^T X^{(\mathrm{train})T} \mathbf{y}^{(\mathrm{train})} + \mathbf{y}^{(\mathrm{train})T} \mathbf{y}^{(\mathrm{train})} \right\}] +
+
stem:[\Rightarrow 2 X^{(\mathrm{train})T} X^{(\mathrm{train})} \mathbf{w} - 2 X^{(\mathrm{train})T} \mathbf{y}^{(\mathrm{train})} = 0] +
+
stem:[\Rightarrow \hat{\mathbf{w}} = \left( X^{(\mathrm{train})T} X^{(\mathrm{train})} \right)^{-1} X^{(\mathrm{train})T} \mathbf{y}^{(\mathrm{train})}]

回帰係数::
+
[stem]
++++
\hat{\mathbf{w}} =
\left(
X^{(\mathrm{train})T} X^{(\mathrm{train})}
\right)^{-1}
X^{(\mathrm{train})T} \mathbf{y}^{(\mathrm{train})}
++++

予測値::
[stem]
++++
\hat{\mathbf{y}} =
X
\left(
X^{(\mathrm{train})T} X^{(\mathrm{train})}
\right)^{-1}
X^{(\mathrm{train})T} \mathbf{y}^{(\mathrm{train})}
++++

新しい入力stem:[X_∗]に対する予測::
[stem]
++++
\hat{\mathbf{y}} =
\underbrace{X_*}_{n_x \times (m+1)}
\cdot
\underbrace{\hat{\mathbf{w}}}_{(m+1) \times 1}
=
\underbrace{
X_*
(X^T X)^{-1} X^T
}_{(m+1) \times 1}
\mathbf{y}
++++

[stem]
++++
X_* \cdot \hat{\mathbf{w}}
=
X_* \cdot (X^T X)^{-1} X^T
\text{を射影行列という}
++++

=== ハンズオン
課題::
ボストンの住宅価格予測

考察::
* 部屋数が増えれば住宅価格が上がる
* 犯罪率が下がれば住宅価格が上がる +
→一般的に考えると、「犯罪率を高くすると価格は下がり、部屋数を多くすると価格があがる」ため、モデルが妥当と思われる +
→部屋数の方が住宅価格への影響は大きい +

データセットの注意点::
* 不正な値がないか確認する(範囲外の値、外れ値など)
* ヒストグラムにしてみる
* 平均値、最大値、最小値を確認する
* 最初数行確認してみて、気になる点があれば行数を増やして確認するなど

予測の値が明らかにおかしい場合::
* モデルの設計ミス(直線ではなく指数関数で予測するなど)
* 元のデータセットにない値を使っていないか
* ドメイン知識に基づいて予測結果を確認する


== 第4章 非線形回帰モデル

=== 非線形回帰モデルとは

* 入力と出力の関係が直線では表せない場合に用いる回帰手法 
* 曲線・指数関数・対数関数・多項式など、より柔軟な関数でデータを近似する 
* 線形回帰では表現できない複雑なパターンを捉えることが可能で、入力変数に非線形変換を施す 
* ただし、**パラメータ w に対しては線形**であることが重要（線形モデルの枠組みを保つ） 

// リスト終了を明示
[none]

モデル例:: 
* 単回帰：stem:[y = w_0 + w_1 \cdot x] 
* 重回帰：stem:[y = w_0 + w_1 \cdot x_1 + w_2 \cdot x_2 + \cdots + w_m \cdot x_m] 

非線形変換の例:: 
* 多項式回帰：stem:[y = w_0 + w_1 \cdot x + w_2 \cdot x^2 + w_3 \cdot x^3] 
* 関数変換：stem:[y = w_0 + w_1 \cdot \sin x + w_2 \cdot \cos x + w_3 \cdot \log x] 

特徴量変換:: 
* 入力 stem:[x] を任意の関数 stem:[\phi(x)] に変換することで非線形性を導入 
* ただし、**モデルは w に対して線形（linear-in-parameter）**である +
stem:[\hat{y} = w_0 + w_1 \cdot \phi_1(x) + w_2 \cdot \phi_2(x) + \cdots + w_m \cdot \phi_m(x)]

線形性の保持:: 
* stem:[f(x) = ax^2 + bx + c] は stem:[x] に対して2次関数→非線形 
* stem:[f(a) = ax^2 + bx + c] は stem:[a] に対して1次関数→線形
+
→ モデルの出力が w に対して線形(linear-in-parameter)であれば、最小二乗法などの解析的手法が適用可能 

注意点:: 
* 過学習のリスク（高次変換や複雑な関数を使いすぎると汎化性能が低下） 
* 特徴量変換の選択はドメイン知識や交差検証に基づいて慎重に行う


=== 基底関数
* 非線形回帰モデルは φ(x) を使うことで w に対して線形になる
* その φ(x) をどう作るか？を決めるのが基底関数
* つまり、基底関数は非線形回帰モデルそのものを定義する部品

// リスト終了を明示
[none]


モデル式::
+
[stem]
++++
y_i = f(x_i) + \varepsilon_i
++++
+
* 回帰関数 stem:[f(x)] を基底関数の線形結合で表す  
+
[stem]
++++
y_i = w_0 + \sum_{j=1}^{m} w_j \phi_j(x_i) + \varepsilon_i
++++

// リスト終了を明示
[none]

特徴::
* 入力 stem:[x] に対して非線形な変換 stem:[\phi_j(x)] を施す
* 出力は基底関数の線形結合として表現される
* パラメータ stem:[\mathbf{w}] に対しては線形（linear-in-parameter）

代表的な基底関数::
* 多項式関数：stem:[\phi_j(x) = x^j]
* ガウス型基底関数（RBF）：stem:[\phi_j(x) = \exp\left( -\frac{(x - \mu_j)^2}{2\sigma^2} \right)]
* スプライン関数 / Bスプライン関数：区分的多項式で滑らかな曲線を構成

多項式関数の例::
+
[stem]
++++
\hat{y}_i = w_0 + w_1 \cdot \phi_1(x_i) + w_2 \cdot \phi_2(x_i) + \cdots + w_p \cdot \phi_p(x_i)
= w_0 + w_1 \cdot x_i + w_2 \cdot x_i^2 + \cdots + w_p \cdot x_i^p
++++
+
* 多項式基底：stem:[\phi_j(x) = x^j]
* パラメータ stem:[w_j] に対しては線形

ガウス型基底関数の例（1次元）::
+
[stem]
++++
\phi_j(x) = \exp\left( -\frac{(x - \mu_j)^2}{2 h_j} \right)
= \exp\left( -\frac{(x - \mu_j)^2}{\sigma^2} \right)
++++

ガウス型基底関数の例（多次元）::
+
[stem]
++++
\phi_j(x) = \exp\left( -\frac{(x - \mu_j)^T (x - \mu_j)}{2 h_j} \right)
++++

=== 特徴量表現

説明変数::
+
stem:[
x_i = (x_{i1}, x_{i2}, \dots, x_{im}) \in \mathbb{R}^m
]
+
* m次元の入力ベクトル

非線形関数ベクトル::
+
stem:[
\phi(x_i) = (\phi_1(x_i), \phi_2(x_i), \dots, \phi_k(x_i))^T \in \mathbb{R}^k
]
+
* k個の基底関数で構成される特徴ベクトル
* 多項式基底の例 stem:[\quad \phi(x_i) = (x_i,\ x_i^2,\ x_i^3,\ \dots,\ x_i^p)^T ]

計画行列（訓練データ）::
+
stem:[
\Phi_{\mathrm{train}} = (\phi(x_1), \phi(x_2), \dots, \phi(x_n))^T \in \mathbb{R}^{n \times k}
]

予測値（最小二乗法）::
+
[stem]
++++
\hat{y}
=
\Phi_{\mathrm{test}}
\left(
\Phi_{\mathrm{train}}^T \Phi_{\mathrm{train}}
\right)^{-1}
\Phi_{\mathrm{train}}^T
\mathbf{y}_{\mathrm{train}}
++++

=== 未学習と過学習（Underfitting / Overfitting）

未学習::
* 学習データに対して十分小さな誤差が得られない状態
* モデルの表現力が不足しており、データの構造を捉えきれていない
+
* 対策
** より表現力の高いモデルを利用する
*** 高次の基底関数を導入
*** モデルの自由度を増やす

// リスト終了を明示
[none]

過学習::
* 学習データに対しては小さな誤差が得られるが、テストデータでは誤差が大きくなる状態
* モデルが学習データに過剰に適合し、汎化性能が低下している
+
* 対策
** 学習データの数を増やす
** 不要な基底関数（変数）を削除して表現力を抑制する
*** 特徴量選択
*** ドメイン知識に基づく変数削除
** AICによるモデル選択
** 正則化法を利用して表現力を抑制する
*** L1/L2正則化
*** ベイズ的アプローチ

モデルの複雑さと基底関数の選定::
* モデルの複雑さは、基底関数の数・位置・バンド幅により変化する
* 適切な基底関数の選定が重要（交差検証などで選択）
* 解きたい問題に対して基底関数が多すぎると過学習のリスクが高まる

=== 正則化（Regularization）
* モデルの複雑さに伴い、パラメータ stem:[w] が暴走するのを防ぐ
* 過学習を抑制し、汎化性能を向上させる
* 数値的に不安定な逆行列を安定化させる

// リスト終了を明示
[none]

正則化付き目的関数::
+
[stem]
++++
S_{\gamma} = (y - \Phi w)^T (y - \Phi w) + \gamma R(w) \quad (\gamma > 0)
++++
* 構成
** stem:[(y - \Phi w)^T (y - \Phi w)]：予測誤差（二乗誤差）
** stem:[R(w)]：正則化項（罰則項）
** stem:[\gamma]：正則化パラメータ（誤差と罰則のバランスを調整）
+
* 意味
** 誤差を減らしつつ、モデルの複雑さに罰則を与えることで、過学習を防ぐ
** stem:[\gamma] が大きい → モデルを滑らかに（複雑さ抑制）
** stem:[\gamma] が小さい → 誤差優先（複雑なモデル）

正則化項の種類と効果::
[cols="1,1,2,2", options="header"]
|===
| 正則化手法 | 罰則項 stem:[R(w)] | 効果 | 向いている場面

| L2（Ridge）
| stem:[\sum w_i^2]
| 重みを小さく抑える（ゼロにはしない）  
数値安定性が高く、滑らかなモデルになる
| 全体の特徴量を活かしたいとき  
多重共線性のある場合

| L1（Lasso）
| stem:[\sum |w_i|]
| 一部の重みをゼロにする  
特徴選択が自動で行われる
| 高次元・疎なモデルを作りたいとき  
不要な特徴量を削除したいとき
|===

数値的不安定性と正則化の必要性::
+
* 線形回帰の解析解
+
[stem]
++++
\hat{y} = X_{\mathrm{star}} \cdot \underbrace{(X^T X)^{-1} X^T y}_{\hat{w}}
++++
+
* 問題点
** stem:[X] の列ベクトルが「ほぼ平行」な場合、stem:[X^T X] が特異に近くなる
** 逆行列 stem:[(X^T X)^{-1}] の要素が極端に大きくなり、数値的に不安定
** 推定されたパラメータ stem:[\hat{w}] が爆発的に大きくなる可能性がある
+
.例（計画行列）各列がほぼ線形従属 → 多重共線性の発生
[stem]
++++
X = \begin{pmatrix}
1 & 2 & 4 \\
1 & 3 & 5.9 \\
1 & 4 & 8.1
\end{pmatrix}
++++
+
* 対策（L2正則化）
+
[stem]
++++
\hat{w} = (X^T X + \lambda I)^{-1} X^T y
++++
+
** stem:[\lambda I] により対角成分が補強され、逆行列が安定化
** 数値誤差の抑制、過学習の防止、汎化性能の向上につながる

=== データの分割とモデルの汎化性能測定

ホールドアウト法（Hold-out Method）::
+
* データを「学習用」と「評価用」に一度だけ分割して性能を測る方法
+
* 手順
** データ集合 stem:[D] を学習データ stem:[D_{\mathrm{train}}] とテストデータ stem:[D_{\mathrm{test}}] に分割
** 学習データでモデルを学習
** テストデータで汎化性能を評価
+
* 特徴
** 計算コストが低い
** 分割の仕方により評価値が大きく変動する（不安定）
** データ量が少ない場合は特に不利
+
* 用途
** データ量が十分に多い場合の簡易評価  
** モデル選択の初期段階

交差検証（Cross Validation, CV）::
+
* データを複数回分割し、平均的な性能を評価する方法  
* ホールドアウト法の「分割の偏り」を減らす
+
* 手順
** データ集合 stem:[D] を stem:[K] 個のブロックに分割
** 各ブロックを1回ずつテストデータとして使用し、残りを学習データにする
** stem:[K] 回の評価値を平均して汎化性能とする
+
* 特徴
** 評価が安定し、分割の偏りに強い
** データ量が少ない場合でも有効
** 計算コストはホールドアウト法より高い

ホールドアウト法と交差検証の比較::
+
[cols="1,2,2", options="header"]
|===
| 手法 | 長所 | 短所

| ホールドアウト法
| 計算が速い  
実装が簡単
| 評価が不安定  
データが少ないと性能が落ちる

| 交差検証
| 評価が安定  
データを有効活用  
ハイパーパラメータ選択に強い
| 計算コストが高い
|===

ハイパーパラメータの調整方法::
* グリッドサーチ
** 複数のハイパーパラメータ候補を「格子状（グリッド）」に並べ、 **すべての組み合わせを総当たりで試す方法** 
** 正則化パラメータ stem:[\gamma]、基底関数の数、学習率などの最適値を探索する

== 第5章 ロジスティック回帰モデル
=== 分類問題とは
入力ベクトル stem:[x \in \mathbb{R}^m] に対して、離散的なクラスラベル stem:[y] を予測する問題 +
典型的には、2 クラス分類を考える

[stem]
++++
y \in \{0, 1\}
++++

ここで、

* stem:[x = (x_1, x_2, \ldots, x_m)^T] は説明変数（特徴量）
* stem:[y] は目的変数（教師ラベル）
* 教師データ集合は次のように表される

[stem]
++++
\mathcal{D} = \{ (x_i, y_i) \mid i = 1, \ldots, n \}
++++

分類問題の目的は、未知の入力 stem:[x] に対して、そのクラス stem:[y] を正しく推定することである

ロジスティック回帰は、事後確率 stem:[P(y=1 \mid x)] をモデル化する識別的手法である

=== 分類問題におけるモデルの考え方
識別的アプローチ::

生成的アプローチ::

== 第6章 主成分分析、k近傍法、k-means